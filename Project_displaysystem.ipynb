{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariatomy9/Major-Project/blob/hisana/Project_displaysystem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVg-n6neLKqJ",
        "outputId": "3a28a044-e74f-4788-8d9a-05cea8504205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7rSjfzlh-_f"
      },
      "source": [
        "Epilepsy Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YOCpZ1Vm6cfW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DcTf8OlHqoxP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YxLrf-UAqo0x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b6c16a3-d3a9-4828-a755-9f2a3e46313a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com] [1 InRelease 14.2 kB/114 kB 12%] [Connect\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2,270 kB]\n",
            "Get:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease [18.1 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,721 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:13 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,344 kB]\n",
            "Get:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 Packages [29.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,197 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,408 kB]\n",
            "Fetched 12.3 MB in 4s (3,496 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libblkid-dev libcairo-script-interpreter2 libglib2.0-dev libglib2.0-dev-bin\n",
            "  liblzo2-2 libmount-dev libpixman-1-dev libselinux1-dev libsepol1-dev\n",
            "  libxcb-render0-dev libxcb-shm0-dev\n",
            "Suggested packages:\n",
            "  libcairo2-doc libgirepository1.0-dev libglib2.0-doc libgdk-pixbuf2.0-bin\n",
            "  | libgdk-pixbuf2.0-dev libxml2-utils\n",
            "The following NEW packages will be installed:\n",
            "  libblkid-dev libcairo-script-interpreter2 libcairo2-dev libffi-dev\n",
            "  libglib2.0-dev libglib2.0-dev-bin liblzo2-2 libmount-dev libpixman-1-dev\n",
            "  libselinux1-dev libsepol1-dev libxcb-render0-dev libxcb-shm0-dev\n",
            "0 upgraded, 13 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 3,492 kB of archives.\n",
            "After this operation, 20.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 liblzo2-2 amd64 2.10-2 [50.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libcairo-script-interpreter2 amd64 1.16.0-4ubuntu1 [54.2 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpixman-1-dev amd64 0.38.4-0ubuntu2.1 [243 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-render0-dev amd64 1.14-2 [18.4 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-shm0-dev amd64 1.14-2 [6,716 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 libffi-dev amd64 3.3-4 [57.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libglib2.0-dev-bin amd64 2.64.6-1~ubuntu20.04.4 [109 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libblkid-dev amd64 2.34-0.1ubuntu9.3 [167 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libmount-dev amd64 2.34-0.1ubuntu9.3 [176 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libsepol1-dev amd64 3.0-1ubuntu0.1 [325 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 libselinux1-dev amd64 3.0-1build2 [151 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libglib2.0-dev amd64 2.64.6-1~ubuntu20.04.4 [1,506 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 libcairo2-dev amd64 1.16.0-4ubuntu1 [627 kB]\n",
            "Fetched 3,492 kB in 2s (1,862 kB/s)\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "(Reading database ... 122531 files and directories currently installed.)\n",
            "Preparing to unpack .../00-liblzo2-2_2.10-2_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2) ...\n",
            "Selecting previously unselected package libcairo-script-interpreter2:amd64.\n",
            "Preparing to unpack .../01-libcairo-script-interpreter2_1.16.0-4ubuntu1_amd64.deb ...\n",
            "Unpacking libcairo-script-interpreter2:amd64 (1.16.0-4ubuntu1) ...\n",
            "Selecting previously unselected package libpixman-1-dev:amd64.\n",
            "Preparing to unpack .../02-libpixman-1-dev_0.38.4-0ubuntu2.1_amd64.deb ...\n",
            "Unpacking libpixman-1-dev:amd64 (0.38.4-0ubuntu2.1) ...\n",
            "Selecting previously unselected package libxcb-render0-dev:amd64.\n",
            "Preparing to unpack .../03-libxcb-render0-dev_1.14-2_amd64.deb ...\n",
            "Unpacking libxcb-render0-dev:amd64 (1.14-2) ...\n",
            "Selecting previously unselected package libxcb-shm0-dev:amd64.\n",
            "Preparing to unpack .../04-libxcb-shm0-dev_1.14-2_amd64.deb ...\n",
            "Unpacking libxcb-shm0-dev:amd64 (1.14-2) ...\n",
            "Selecting previously unselected package libffi-dev:amd64.\n",
            "Preparing to unpack .../05-libffi-dev_3.3-4_amd64.deb ...\n",
            "Unpacking libffi-dev:amd64 (3.3-4) ...\n",
            "Selecting previously unselected package libglib2.0-dev-bin.\n",
            "Preparing to unpack .../06-libglib2.0-dev-bin_2.64.6-1~ubuntu20.04.4_amd64.deb ...\n",
            "Unpacking libglib2.0-dev-bin (2.64.6-1~ubuntu20.04.4) ...\n",
            "Selecting previously unselected package libblkid-dev:amd64.\n",
            "Preparing to unpack .../07-libblkid-dev_2.34-0.1ubuntu9.3_amd64.deb ...\n",
            "Unpacking libblkid-dev:amd64 (2.34-0.1ubuntu9.3) ...\n",
            "Selecting previously unselected package libmount-dev:amd64.\n",
            "Preparing to unpack .../08-libmount-dev_2.34-0.1ubuntu9.3_amd64.deb ...\n",
            "Unpacking libmount-dev:amd64 (2.34-0.1ubuntu9.3) ...\n",
            "Selecting previously unselected package libsepol1-dev:amd64.\n",
            "Preparing to unpack .../09-libsepol1-dev_3.0-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libsepol1-dev:amd64 (3.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libselinux1-dev:amd64.\n",
            "Preparing to unpack .../10-libselinux1-dev_3.0-1build2_amd64.deb ...\n",
            "Unpacking libselinux1-dev:amd64 (3.0-1build2) ...\n",
            "Selecting previously unselected package libglib2.0-dev:amd64.\n",
            "Preparing to unpack .../11-libglib2.0-dev_2.64.6-1~ubuntu20.04.4_amd64.deb ...\n",
            "Unpacking libglib2.0-dev:amd64 (2.64.6-1~ubuntu20.04.4) ...\n",
            "Selecting previously unselected package libcairo2-dev:amd64.\n",
            "Preparing to unpack .../12-libcairo2-dev_1.16.0-4ubuntu1_amd64.deb ...\n",
            "Unpacking libcairo2-dev:amd64 (1.16.0-4ubuntu1) ...\n",
            "Setting up libglib2.0-dev-bin (2.64.6-1~ubuntu20.04.4) ...\n",
            "Setting up libblkid-dev:amd64 (2.34-0.1ubuntu9.3) ...\n",
            "Setting up libpixman-1-dev:amd64 (0.38.4-0ubuntu2.1) ...\n",
            "Setting up libsepol1-dev:amd64 (3.0-1ubuntu0.1) ...\n",
            "Setting up liblzo2-2:amd64 (2.10-2) ...\n",
            "Setting up libffi-dev:amd64 (3.3-4) ...\n",
            "Setting up libxcb-shm0-dev:amd64 (1.14-2) ...\n",
            "Setting up libxcb-render0-dev:amd64 (1.14-2) ...\n",
            "Setting up libmount-dev:amd64 (2.34-0.1ubuntu9.3) ...\n",
            "Setting up libcairo-script-interpreter2:amd64 (1.16.0-4ubuntu1) ...\n",
            "Setting up libselinux1-dev:amd64 (3.0-1build2) ...\n",
            "Setting up libglib2.0-dev:amd64 (2.64.6-1~ubuntu20.04.4) ...\n",
            "Processing triggers for libglib2.0-0:amd64 (2.64.6-1~ubuntu20.04.4) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Setting up libcairo2-dev:amd64 (1.16.0-4ubuntu1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting reportlab\n",
            "  Downloading reportlab-4.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow>=9.0.0 (from reportlab)\n",
            "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rlPyCairo<1,>=0.2.0 (from reportlab)\n",
            "  Downloading rlPyCairo-0.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting freetype-py<2.4,>=2.3.0 (from reportlab)\n",
            "  Downloading freetype_py-2.3.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (978 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.9/978.9 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycairo>=1.20.0 (from rlPyCairo<1,>=0.2.0->reportlab)\n",
            "  Downloading pycairo-1.23.0.tar.gz (344 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.6/344.6 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pycairo\n",
            "  Building wheel for pycairo (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycairo: filename=pycairo-1.23.0-cp310-cp310-linux_x86_64.whl size=331628 sha256=9c68a8dce3e22ff99be88c302f1ef091d4266d5c152eb571489e4ce69fdce28a\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/63/63/4341460df2dca6490f16a23996298f277a3441f0b08bebe69b\n",
            "Successfully built pycairo\n",
            "Installing collected packages: pycairo, pillow, freetype-py, rlPyCairo, reportlab\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "Successfully installed freetype-py-2.3.0 pillow-9.5.0 pycairo-1.23.0 reportlab-4.0.0 rlPyCairo-0.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!apt-get update && apt-get install -y libffi-dev libcairo2-dev\n",
        "!pip install reportlab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U3X7cWfqsTI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5820dd33-d4c2-4611-b5b8-1a611231e8ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: eeglib in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from eeglib) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from eeglib) (1.10.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.10/dist-packages (from eeglib) (0.0.post5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from eeglib) (0.56.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from eeglib) (1.5.3)\n",
            "Requirement already satisfied: pyedflib in /usr/local/lib/python3.10/dist-packages (from eeglib) (0.1.32)\n",
            "Requirement already satisfied: fastdtw in /usr/local/lib/python3.10/dist-packages (from eeglib) (0.3.4)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.10/dist-packages (from eeglib) (4.2.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->eeglib) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->eeglib) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->eeglib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->eeglib) (2022.7.1)\n",
            "Requirement already satisfied: python-utils>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from progressbar2->eeglib) (3.5.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->eeglib) (1.16.0)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install eeglib\n",
        "\n",
        "!pip install -q pyngrok\n",
        "!pip install -q streamlit\n",
        "!pip install -q streamlit_ace\n",
        "!pip install PyPDF2\n",
        "!pip install fpdf\n",
        "!pip install mne\n",
        "!pip install pyEDFlib\n",
        "!pip install EDFlib-Python\n",
        "# !pip install fpdf\n",
        "# !pip install reportlab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lh9bzEY0qsYu"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import glob\n",
        "import librosa\n",
        "import numpy as np\n",
        "import librosa.display\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import loadtxt\n",
        "import seaborn as sn\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxxeduAVqscz"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import  ModelCheckpoint\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4SXGHgTqsgL"
      },
      "outputs": [],
      "source": [
        "epilepsy_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/original_epilepsy_types_new6.csv')\n",
        "epilepsy_data =epilepsy_data.drop(['Unnamed: 0'],axis=1)\n",
        "epilepsy_data.columns=['PFD0', 'PFD1', 'PFD2', 'PFD3', 'PFD4', 'PFD5', 'PFD6', 'PFD7', 'PFD8',\n",
        "       'PFD9', 'PFD10', 'PFD11', 'PFD12', 'PFD13', 'PFD14', 'PFD15', 'PFD16',\n",
        "       'PFD17', 'PFD18', 'PFD19', 'PFD20', 'PFD21', 'PFD22', 'DTW',\n",
        "       'hjorthActivity0', 'hjorthActivity1', 'hjorthActivity2',\n",
        "       'hjorthActivity3', 'hjorthActivity4', 'hjorthActivity5',\n",
        "       'hjorthActivity6', 'hjorthActivity7', 'hjorthActivity8',\n",
        "       'hjorthActivity9', 'hjorthActivity10', 'hjorthActivity11',\n",
        "       'hjorthActivity12', 'hjorthActivity13', 'hjorthActivity14',\n",
        "       'hjorthActivity15', 'hjorthActivity16', 'hjorthActivity17',\n",
        "       'hjorthActivity18', 'hjorthActivity19', 'hjorthActivity20',\n",
        "       'hjorthActivity21', 'hjorthActivity22','Label']\n",
        "x=epilepsy_data[['PFD0', 'PFD1', 'PFD2', 'PFD3', 'PFD4', 'PFD5', 'PFD6', 'PFD7', 'PFD8',\n",
        "       'PFD9', 'PFD10', 'PFD11', 'PFD12', 'PFD13', 'PFD14', 'PFD15', 'PFD16',\n",
        "       'PFD17', 'PFD18', 'PFD19', 'PFD20', 'PFD21', 'PFD22', 'DTW',\n",
        "       'hjorthActivity0', 'hjorthActivity1', 'hjorthActivity2',\n",
        "       'hjorthActivity3', 'hjorthActivity4', 'hjorthActivity5',\n",
        "       'hjorthActivity6', 'hjorthActivity7', 'hjorthActivity8',\n",
        "       'hjorthActivity9', 'hjorthActivity10', 'hjorthActivity11',\n",
        "       'hjorthActivity12', 'hjorthActivity13', 'hjorthActivity14',\n",
        "       'hjorthActivity15', 'hjorthActivity16', 'hjorthActivity17',\n",
        "       'hjorthActivity18', 'hjorthActivity19', 'hjorthActivity20',\n",
        "       'hjorthActivity21', 'hjorthActivity22']]\n",
        "y = epilepsy_data.loc[:,'Label'].values\n",
        "print(x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMdhuUnTqo4A"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import pandas as pd\n",
        "import eeglib\n",
        "from eeglib.helpers import EDFHelper\n",
        "import mne\n",
        "from collections import Counter\n",
        "import statistics\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import glob\n",
        "import librosa\n",
        "import numpy as np\n",
        "import librosa.display\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import loadtxt\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import GRU,LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import  ModelCheckpoint\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEUHZQDit2z5",
        "outputId": "e3175b14-0381-46b7-ea88-69b9ed47638d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "709/709 [==============================] - 12s 9ms/step - loss: 1.4422 - accuracy: 0.4911 - val_loss: 0.9048 - val_accuracy: 0.7444\n",
            "Epoch 2/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.7753 - accuracy: 0.7544 - val_loss: 0.5615 - val_accuracy: 0.8465\n",
            "Epoch 3/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.5710 - accuracy: 0.8189 - val_loss: 0.4263 - val_accuracy: 0.8762\n",
            "Epoch 4/100\n",
            "709/709 [==============================] - 6s 9ms/step - loss: 0.4792 - accuracy: 0.8506 - val_loss: 0.3501 - val_accuracy: 0.9016\n",
            "Epoch 5/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.4132 - accuracy: 0.8744 - val_loss: 0.3029 - val_accuracy: 0.9148\n",
            "Epoch 6/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.3711 - accuracy: 0.8869 - val_loss: 0.2741 - val_accuracy: 0.9199\n",
            "Epoch 7/100\n",
            "709/709 [==============================] - 5s 8ms/step - loss: 0.3439 - accuracy: 0.8937 - val_loss: 0.2509 - val_accuracy: 0.9261\n",
            "Epoch 8/100\n",
            "709/709 [==============================] - 5s 7ms/step - loss: 0.3177 - accuracy: 0.9017 - val_loss: 0.2312 - val_accuracy: 0.9323\n",
            "Epoch 9/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.2996 - accuracy: 0.9061 - val_loss: 0.2147 - val_accuracy: 0.9379\n",
            "Epoch 10/100\n",
            "709/709 [==============================] - 5s 7ms/step - loss: 0.2813 - accuracy: 0.9123 - val_loss: 0.2022 - val_accuracy: 0.9400\n",
            "Epoch 11/100\n",
            "709/709 [==============================] - 6s 9ms/step - loss: 0.2693 - accuracy: 0.9177 - val_loss: 0.1906 - val_accuracy: 0.9444\n",
            "Epoch 12/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.2562 - accuracy: 0.9202 - val_loss: 0.1805 - val_accuracy: 0.9476\n",
            "Epoch 13/100\n",
            "709/709 [==============================] - 6s 8ms/step - loss: 0.2361 - accuracy: 0.9257 - val_loss: 0.1710 - val_accuracy: 0.9503\n",
            "Epoch 14/100\n",
            "709/709 [==============================] - 6s 9ms/step - loss: 0.2292 - accuracy: 0.9286 - val_loss: 0.1632 - val_accuracy: 0.9525\n",
            "Epoch 15/100\n",
            "709/709 [==============================] - 7s 11ms/step - loss: 0.2162 - accuracy: 0.9312 - val_loss: 0.1565 - val_accuracy: 0.9534\n",
            "Epoch 16/100\n",
            "709/709 [==============================] - 7s 11ms/step - loss: 0.2063 - accuracy: 0.9339 - val_loss: 0.1491 - val_accuracy: 0.9552\n",
            "Epoch 17/100\n",
            "709/709 [==============================] - 5s 7ms/step - loss: 0.2044 - accuracy: 0.9361 - val_loss: 0.1430 - val_accuracy: 0.9582\n",
            "Epoch 18/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.1994 - accuracy: 0.9382 - val_loss: 0.1370 - val_accuracy: 0.9598\n",
            "Epoch 19/100\n",
            "709/709 [==============================] - 6s 8ms/step - loss: 0.1845 - accuracy: 0.9409 - val_loss: 0.1305 - val_accuracy: 0.9612\n",
            "Epoch 20/100\n",
            "709/709 [==============================] - 5s 7ms/step - loss: 0.1771 - accuracy: 0.9441 - val_loss: 0.1266 - val_accuracy: 0.9623\n",
            "Epoch 21/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.1746 - accuracy: 0.9444 - val_loss: 0.1241 - val_accuracy: 0.9617\n",
            "Epoch 22/100\n",
            "709/709 [==============================] - 5s 7ms/step - loss: 0.1701 - accuracy: 0.9446 - val_loss: 0.1192 - val_accuracy: 0.9642\n",
            "Epoch 23/100\n",
            "709/709 [==============================] - 6s 8ms/step - loss: 0.1682 - accuracy: 0.9457 - val_loss: 0.1147 - val_accuracy: 0.9652\n",
            "Epoch 24/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.1644 - accuracy: 0.9491 - val_loss: 0.1108 - val_accuracy: 0.9658\n",
            "Epoch 25/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.1545 - accuracy: 0.9503 - val_loss: 0.1075 - val_accuracy: 0.9677\n",
            "Epoch 26/100\n",
            "709/709 [==============================] - 6s 9ms/step - loss: 0.1473 - accuracy: 0.9521 - val_loss: 0.1068 - val_accuracy: 0.9677\n",
            "Epoch 27/100\n",
            "709/709 [==============================] - 7s 10ms/step - loss: 0.1462 - accuracy: 0.9530 - val_loss: 0.1016 - val_accuracy: 0.9691\n",
            "Epoch 28/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.1406 - accuracy: 0.9551 - val_loss: 0.1011 - val_accuracy: 0.9704\n",
            "Epoch 29/100\n",
            "709/709 [==============================] - 6s 9ms/step - loss: 0.1385 - accuracy: 0.9565 - val_loss: 0.0983 - val_accuracy: 0.9698\n",
            "Epoch 30/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.1338 - accuracy: 0.9575 - val_loss: 0.0978 - val_accuracy: 0.9709\n",
            "Epoch 31/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.1331 - accuracy: 0.9563 - val_loss: 0.0934 - val_accuracy: 0.9723\n",
            "Epoch 32/100\n",
            "709/709 [==============================] - 6s 9ms/step - loss: 0.1324 - accuracy: 0.9581 - val_loss: 0.0910 - val_accuracy: 0.9721\n",
            "Epoch 33/100\n",
            "709/709 [==============================] - 5s 7ms/step - loss: 0.1281 - accuracy: 0.9600 - val_loss: 0.0903 - val_accuracy: 0.9725\n",
            "Epoch 34/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.1276 - accuracy: 0.9587 - val_loss: 0.0878 - val_accuracy: 0.9742\n",
            "Epoch 35/100\n",
            "709/709 [==============================] - 7s 10ms/step - loss: 0.1216 - accuracy: 0.9604 - val_loss: 0.0887 - val_accuracy: 0.9739\n",
            "Epoch 36/100\n",
            "709/709 [==============================] - 7s 10ms/step - loss: 0.1172 - accuracy: 0.9621 - val_loss: 0.0857 - val_accuracy: 0.9735\n",
            "Epoch 37/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.1140 - accuracy: 0.9634 - val_loss: 0.0831 - val_accuracy: 0.9748\n",
            "Epoch 38/100\n",
            "709/709 [==============================] - 5s 6ms/step - loss: 0.1135 - accuracy: 0.9626 - val_loss: 0.0830 - val_accuracy: 0.9741\n",
            "Epoch 39/100\n",
            "709/709 [==============================] - 7s 9ms/step - loss: 0.1129 - accuracy: 0.9626 - val_loss: 0.0793 - val_accuracy: 0.9753\n",
            "Epoch 40/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.1145 - accuracy: 0.9621 - val_loss: 0.0792 - val_accuracy: 0.9757\n",
            "Epoch 41/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.1103 - accuracy: 0.9645 - val_loss: 0.0771 - val_accuracy: 0.9765\n",
            "Epoch 42/100\n",
            "709/709 [==============================] - 6s 8ms/step - loss: 0.1100 - accuracy: 0.9636 - val_loss: 0.0774 - val_accuracy: 0.9762\n",
            "Epoch 43/100\n",
            "709/709 [==============================] - 5s 7ms/step - loss: 0.1057 - accuracy: 0.9652 - val_loss: 0.0756 - val_accuracy: 0.9767\n",
            "Epoch 44/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.1075 - accuracy: 0.9651 - val_loss: 0.0755 - val_accuracy: 0.9772\n",
            "Epoch 45/100\n",
            "709/709 [==============================] - 5s 7ms/step - loss: 0.1021 - accuracy: 0.9684 - val_loss: 0.0740 - val_accuracy: 0.9772\n",
            "Epoch 46/100\n",
            "709/709 [==============================] - 6s 9ms/step - loss: 0.1007 - accuracy: 0.9672 - val_loss: 0.0734 - val_accuracy: 0.9783\n",
            "Epoch 47/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0959 - accuracy: 0.9686 - val_loss: 0.0724 - val_accuracy: 0.9776\n",
            "Epoch 48/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0964 - accuracy: 0.9683 - val_loss: 0.0694 - val_accuracy: 0.9781\n",
            "Epoch 49/100\n",
            "709/709 [==============================] - 7s 9ms/step - loss: 0.0967 - accuracy: 0.9668 - val_loss: 0.0697 - val_accuracy: 0.9772\n",
            "Epoch 50/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0940 - accuracy: 0.9700 - val_loss: 0.0689 - val_accuracy: 0.9790\n",
            "Epoch 51/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0914 - accuracy: 0.9703 - val_loss: 0.0681 - val_accuracy: 0.9785\n",
            "Epoch 52/100\n",
            "709/709 [==============================] - 6s 8ms/step - loss: 0.0906 - accuracy: 0.9709 - val_loss: 0.0660 - val_accuracy: 0.9790\n",
            "Epoch 53/100\n",
            "709/709 [==============================] - 5s 7ms/step - loss: 0.0895 - accuracy: 0.9706 - val_loss: 0.0666 - val_accuracy: 0.9799\n",
            "Epoch 54/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0855 - accuracy: 0.9708 - val_loss: 0.0655 - val_accuracy: 0.9797\n",
            "Epoch 55/100\n",
            "709/709 [==============================] - 5s 8ms/step - loss: 0.0880 - accuracy: 0.9723 - val_loss: 0.0658 - val_accuracy: 0.9804\n",
            "Epoch 56/100\n",
            "709/709 [==============================] - 6s 9ms/step - loss: 0.0836 - accuracy: 0.9726 - val_loss: 0.0642 - val_accuracy: 0.9804\n",
            "Epoch 57/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0823 - accuracy: 0.9722 - val_loss: 0.0658 - val_accuracy: 0.9801\n",
            "Epoch 58/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0884 - accuracy: 0.9706 - val_loss: 0.0640 - val_accuracy: 0.9815\n",
            "Epoch 59/100\n",
            "709/709 [==============================] - 7s 9ms/step - loss: 0.0857 - accuracy: 0.9714 - val_loss: 0.0624 - val_accuracy: 0.9813\n",
            "Epoch 60/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0830 - accuracy: 0.9722 - val_loss: 0.0617 - val_accuracy: 0.9813\n",
            "Epoch 61/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0844 - accuracy: 0.9738 - val_loss: 0.0616 - val_accuracy: 0.9809\n",
            "Epoch 62/100\n",
            "709/709 [==============================] - 6s 9ms/step - loss: 0.0758 - accuracy: 0.9751 - val_loss: 0.0620 - val_accuracy: 0.9829\n",
            "Epoch 63/100\n",
            "709/709 [==============================] - 5s 7ms/step - loss: 0.0754 - accuracy: 0.9747 - val_loss: 0.0634 - val_accuracy: 0.9806\n",
            "Epoch 64/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0779 - accuracy: 0.9743 - val_loss: 0.0601 - val_accuracy: 0.9818\n",
            "Epoch 65/100\n",
            "709/709 [==============================] - 6s 8ms/step - loss: 0.0738 - accuracy: 0.9750 - val_loss: 0.0597 - val_accuracy: 0.9813\n",
            "Epoch 66/100\n",
            "709/709 [==============================] - 5s 8ms/step - loss: 0.0745 - accuracy: 0.9768 - val_loss: 0.0610 - val_accuracy: 0.9831\n",
            "Epoch 67/100\n",
            "709/709 [==============================] - 5s 6ms/step - loss: 0.0710 - accuracy: 0.9762 - val_loss: 0.0619 - val_accuracy: 0.9817\n",
            "Epoch 68/100\n",
            "709/709 [==============================] - 5s 7ms/step - loss: 0.0726 - accuracy: 0.9757 - val_loss: 0.0595 - val_accuracy: 0.9829\n",
            "Epoch 69/100\n",
            "709/709 [==============================] - 6s 9ms/step - loss: 0.0742 - accuracy: 0.9768 - val_loss: 0.0579 - val_accuracy: 0.9838\n",
            "Epoch 70/100\n",
            "709/709 [==============================] - 5s 6ms/step - loss: 0.0757 - accuracy: 0.9744 - val_loss: 0.0577 - val_accuracy: 0.9831\n",
            "Epoch 71/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0721 - accuracy: 0.9767 - val_loss: 0.0566 - val_accuracy: 0.9832\n",
            "Epoch 72/100\n",
            "709/709 [==============================] - 7s 9ms/step - loss: 0.0722 - accuracy: 0.9768 - val_loss: 0.0566 - val_accuracy: 0.9832\n",
            "Epoch 73/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0696 - accuracy: 0.9767 - val_loss: 0.0559 - val_accuracy: 0.9843\n",
            "Epoch 74/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0724 - accuracy: 0.9749 - val_loss: 0.0555 - val_accuracy: 0.9839\n",
            "Epoch 75/100\n",
            "709/709 [==============================] - 7s 10ms/step - loss: 0.0682 - accuracy: 0.9777 - val_loss: 0.0550 - val_accuracy: 0.9832\n",
            "Epoch 76/100\n",
            "709/709 [==============================] - 7s 10ms/step - loss: 0.0722 - accuracy: 0.9759 - val_loss: 0.0542 - val_accuracy: 0.9834\n",
            "Epoch 77/100\n",
            "709/709 [==============================] - 8s 11ms/step - loss: 0.0668 - accuracy: 0.9778 - val_loss: 0.0558 - val_accuracy: 0.9827\n",
            "Epoch 78/100\n",
            "709/709 [==============================] - 8s 11ms/step - loss: 0.0683 - accuracy: 0.9774 - val_loss: 0.0536 - val_accuracy: 0.9850\n",
            "Epoch 79/100\n",
            "709/709 [==============================] - 5s 7ms/step - loss: 0.0667 - accuracy: 0.9791 - val_loss: 0.0530 - val_accuracy: 0.9845\n",
            "Epoch 80/100\n",
            "709/709 [==============================] - 5s 6ms/step - loss: 0.0648 - accuracy: 0.9786 - val_loss: 0.0519 - val_accuracy: 0.9845\n",
            "Epoch 81/100\n",
            "709/709 [==============================] - 6s 8ms/step - loss: 0.0656 - accuracy: 0.9787 - val_loss: 0.0515 - val_accuracy: 0.9847\n",
            "Epoch 82/100\n",
            "709/709 [==============================] - 6s 8ms/step - loss: 0.0642 - accuracy: 0.9786 - val_loss: 0.0522 - val_accuracy: 0.9841\n",
            "Epoch 83/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0711 - accuracy: 0.9765 - val_loss: 0.0531 - val_accuracy: 0.9848\n",
            "Epoch 84/100\n",
            "709/709 [==============================] - 5s 6ms/step - loss: 0.0681 - accuracy: 0.9778 - val_loss: 0.0507 - val_accuracy: 0.9848\n",
            "Epoch 85/100\n",
            "709/709 [==============================] - 6s 9ms/step - loss: 0.0626 - accuracy: 0.9782 - val_loss: 0.0532 - val_accuracy: 0.9839\n",
            "Epoch 86/100\n",
            "709/709 [==============================] - 5s 6ms/step - loss: 0.0649 - accuracy: 0.9782 - val_loss: 0.0512 - val_accuracy: 0.9845\n",
            "Epoch 87/100\n",
            "709/709 [==============================] - 5s 6ms/step - loss: 0.0603 - accuracy: 0.9792 - val_loss: 0.0518 - val_accuracy: 0.9843\n",
            "Epoch 88/100\n",
            "709/709 [==============================] - 7s 10ms/step - loss: 0.0652 - accuracy: 0.9789 - val_loss: 0.0533 - val_accuracy: 0.9831\n",
            "Epoch 89/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0590 - accuracy: 0.9804 - val_loss: 0.0515 - val_accuracy: 0.9839\n",
            "Epoch 90/100\n",
            "709/709 [==============================] - 5s 6ms/step - loss: 0.0626 - accuracy: 0.9802 - val_loss: 0.0511 - val_accuracy: 0.9841\n",
            "Epoch 91/100\n",
            "709/709 [==============================] - 6s 9ms/step - loss: 0.0578 - accuracy: 0.9812 - val_loss: 0.0510 - val_accuracy: 0.9839\n",
            "Epoch 92/100\n",
            "709/709 [==============================] - 5s 7ms/step - loss: 0.0627 - accuracy: 0.9784 - val_loss: 0.0532 - val_accuracy: 0.9841\n",
            "Epoch 93/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0610 - accuracy: 0.9811 - val_loss: 0.0504 - val_accuracy: 0.9850\n",
            "Epoch 94/100\n",
            "709/709 [==============================] - 5s 8ms/step - loss: 0.0603 - accuracy: 0.9810 - val_loss: 0.0516 - val_accuracy: 0.9836\n",
            "Epoch 95/100\n",
            "709/709 [==============================] - 5s 7ms/step - loss: 0.0595 - accuracy: 0.9799 - val_loss: 0.0511 - val_accuracy: 0.9850\n",
            "Epoch 96/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0555 - accuracy: 0.9812 - val_loss: 0.0510 - val_accuracy: 0.9847\n",
            "Epoch 97/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0557 - accuracy: 0.9815 - val_loss: 0.0494 - val_accuracy: 0.9850\n",
            "Epoch 98/100\n",
            "709/709 [==============================] - 7s 9ms/step - loss: 0.0612 - accuracy: 0.9794 - val_loss: 0.0496 - val_accuracy: 0.9850\n",
            "Epoch 99/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0592 - accuracy: 0.9809 - val_loss: 0.0500 - val_accuracy: 0.9848\n",
            "Epoch 100/100\n",
            "709/709 [==============================] - 4s 6ms/step - loss: 0.0582 - accuracy: 0.9818 - val_loss: 0.0497 - val_accuracy: 0.9843\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x)\n",
        "x = scaler.transform(x)\n",
        "from keras.utils import to_categorical\n",
        "y = to_categorical(y)\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
        "x_train1 = np.reshape(x_train, (x_train.shape[0],1,x.shape[1]))\n",
        "x_test1 = np.reshape(x_test, (x_test.shape[0],1,x.shape[1]))\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(1,47),activation=\"sigmoid\",return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(32,activation=\"sigmoid\"))\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(LSTM(100,return_sequences=True))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(LSTM(50))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(7, activation='sigmoid'))\n",
        "from keras.optimizers import SGD\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
        "\n",
        "model.fit(x_train1, y_train, epochs = 100, validation_data= (x_test1, y_test))\n",
        "model.save('epilepsy_types.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9S7pktIfM_u",
        "outputId": "dbbd7adb-376d-4522-f560-57eb18224108"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "178/178 [==============================] - 1s 3ms/step\n",
            "(5669,)\n",
            "(5669,)\n",
            "Training Accuracy: 0.9843005821132474\n",
            "Precision: 0.9842876090065271\n",
            "Recall: 0.9842905044216298\n",
            "F1 Score: 0.9842843442625856\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "pred = model.predict(x_test1)\n",
        "predict_classes = np.argmax(pred,axis=1)\n",
        "expected_classes = np.argmax(y_test,axis=1)\n",
        "print(expected_classes.shape)\n",
        "print(predict_classes.shape)\n",
        "correct = accuracy_score(expected_classes,predict_classes)\n",
        "correct1 = precision_score(expected_classes,predict_classes,average='macro')\n",
        "correct2 = recall_score(expected_classes,predict_classes,average='macro')\n",
        "correct3 = f1_score(expected_classes,predict_classes,average='macro')\n",
        "print(f\"Training Accuracy: {correct}\")\n",
        "print(f\"Precision: {correct1}\")\n",
        "print(f\"Recall: {correct2}\")\n",
        "print(f\"F1 Score: {correct3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CtV-Ybsibzq"
      },
      "source": [
        "Disease Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVuxLg9biS7F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I-YWw4wiat0",
        "outputId": "43658ab9-fd4b-4c11-aed6-8588675484ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "\r0% [Waiting for headers] [1 InRelease 14.2 kB/114 kB 12%] [Connected to cloud.r\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "\r0% [Waiting for headers] [1 InRelease 48.9 kB/114 kB 43%] [Connected to cloud.r\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "\r0% [3 InRelease 14.2 kB/114 kB 12%] [1 InRelease 48.9 kB/114 kB 43%] [Waiting f\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:5 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Fetched 336 kB in 4s (80.9 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libcairo2-dev is already the newest version (1.16.0-4ubuntu1).\n",
            "libffi-dev is already the newest version (3.3-4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 27 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: reportlab in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from reportlab) (9.5.0)\n",
            "Requirement already satisfied: rlPyCairo<1,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from reportlab) (0.2.0)\n",
            "Requirement already satisfied: freetype-py<2.4,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from reportlab) (2.3.0)\n",
            "Requirement already satisfied: pycairo>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from rlPyCairo<1,>=0.2.0->reportlab) (1.23.0)\n"
          ]
        }
      ],
      "source": [
        "!apt-get update && apt-get install -y libffi-dev libcairo2-dev\n",
        "!pip install reportlab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ8Evv4Jiaw6",
        "outputId": "7eb5f5b5-abc4-4b0c-fe64-37f336f99967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: eeglib in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from eeglib) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from eeglib) (1.10.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.10/dist-packages (from eeglib) (0.0.post5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from eeglib) (0.56.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from eeglib) (1.5.3)\n",
            "Requirement already satisfied: pyedflib in /usr/local/lib/python3.10/dist-packages (from eeglib) (0.1.32)\n",
            "Requirement already satisfied: fastdtw in /usr/local/lib/python3.10/dist-packages (from eeglib) (0.3.4)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.10/dist-packages (from eeglib) (4.2.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->eeglib) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->eeglib) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->eeglib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->eeglib) (2022.7.1)\n",
            "Requirement already satisfied: python-utils>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from progressbar2->eeglib) (3.5.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->eeglib) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fpdf in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from mne) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from mne) (1.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mne) (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mne) (4.65.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne) (1.6.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mne) (23.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mne) (3.1.2)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mne) (2.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne) (9.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mne) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyEDFlib in /usr/local/lib/python3.10/dist-packages (0.1.32)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from pyEDFlib) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: EDFlib-Python in /usr/local/lib/python3.10/dist-packages (1.0.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from EDFlib-Python) (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install eeglib\n",
        "\n",
        "!pip install -q pyngrok\n",
        "!pip install -q streamlit\n",
        "!pip install -q streamlit_ace\n",
        "!pip install PyPDF2\n",
        "!pip install fpdf\n",
        "!pip install mne\n",
        "!pip install pyEDFlib\n",
        "!pip install EDFlib-Python\n",
        "# !pip install fpdf\n",
        "# !pip install reportlab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gz77Ubs_iazw"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import glob\n",
        "import librosa\n",
        "import numpy as np\n",
        "import librosa.display\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import loadtxt\n",
        "import seaborn as sn\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJEdS9QPia5s"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import  ModelCheckpoint\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scCoEthvia8X"
      },
      "outputs": [],
      "source": [
        "epilepsy_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/original_disease_types_latest.csv')\n",
        "epilepsy_data =epilepsy_data.drop(['Unnamed: 0'],axis=1)\n",
        "epilepsy_data.columns=['PFD0', 'PFD1', 'PFD2', 'PFD3', 'PFD4', 'PFD5', 'PFD6', 'PFD7', 'PFD8',\n",
        "       'PFD9', 'PFD10', 'PFD11', 'PFD12', 'PFD13', 'PFD14', 'PFD15', 'PFD16',\n",
        "       'PFD17', 'PFD18', 'PFD19', 'PFD20', 'PFD21', 'PFD22', 'DTW',\n",
        "       'hjorthActivity0', 'hjorthActivity1', 'hjorthActivity2',\n",
        "       'hjorthActivity3', 'hjorthActivity4', 'hjorthActivity5',\n",
        "       'hjorthActivity6', 'hjorthActivity7', 'hjorthActivity8',\n",
        "       'hjorthActivity9', 'hjorthActivity10', 'hjorthActivity11',\n",
        "       'hjorthActivity12', 'hjorthActivity13', 'hjorthActivity14',\n",
        "       'hjorthActivity15', 'hjorthActivity16', 'hjorthActivity17',\n",
        "       'hjorthActivity18', 'hjorthActivity19', 'hjorthActivity20',\n",
        "       'hjorthActivity21', 'hjorthActivity22','Label']\n",
        "x=epilepsy_data[['PFD0', 'PFD1', 'PFD2', 'PFD3', 'PFD4', 'PFD5', 'PFD6', 'PFD7', 'PFD8',\n",
        "       'PFD9', 'PFD10', 'PFD11', 'PFD12', 'PFD13', 'PFD14', 'PFD15', 'PFD16',\n",
        "       'PFD17', 'PFD18', 'PFD19', 'PFD20', 'PFD21', 'PFD22', 'DTW',\n",
        "       'hjorthActivity0', 'hjorthActivity1', 'hjorthActivity2',\n",
        "       'hjorthActivity3', 'hjorthActivity4', 'hjorthActivity5',\n",
        "       'hjorthActivity6', 'hjorthActivity7', 'hjorthActivity8',\n",
        "       'hjorthActivity9', 'hjorthActivity10', 'hjorthActivity11',\n",
        "       'hjorthActivity12', 'hjorthActivity13', 'hjorthActivity14',\n",
        "       'hjorthActivity15', 'hjorthActivity16', 'hjorthActivity17',\n",
        "       'hjorthActivity18', 'hjorthActivity19', 'hjorthActivity20',\n",
        "       'hjorthActivity21', 'hjorthActivity22']]\n",
        "y = epilepsy_data.loc[:,'Label'].values\n",
        "print(x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGARAkn1ivp_"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import pandas as pd\n",
        "import eeglib\n",
        "from eeglib.helpers import EDFHelper\n",
        "import mne\n",
        "from collections import Counter\n",
        "import statistics\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import glob\n",
        "import librosa\n",
        "import numpy as np\n",
        "import librosa.display\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import loadtxt\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import GRU,LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import  ModelCheckpoint\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioltoSVgivtM"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x)\n",
        "x = scaler.transform(x)\n",
        "from keras.utils import to_categorical\n",
        "y = to_categorical(y)\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
        "x_train1 = np.reshape(x_train, (x_train.shape[0],1,x.shape[1]))\n",
        "x_test1 = np.reshape(x_test, (x_test.shape[0],1,x.shape[1]))\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(1,47),activation=\"sigmoid\",return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(32,activation=\"sigmoid\"))\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(LSTM(100,return_sequences=True))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(LSTM(50))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(5, activation='sigmoid'))\n",
        "from keras.optimizers import SGD\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
        "\n",
        "model.fit(x_train1, y_train, epochs = 100, validation_data= (x_test1, y_test))\n",
        "model.save('disease_types.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbattF-vivwU",
        "outputId": "b3715a99-10f7-49aa-b5cf-092085314b94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1063/1063 [==============================] - 2s 2ms/step\n",
            "(34002,)\n",
            "(34002,)\n",
            "Training Accuracy: 0.9958825951414623\n",
            "Precision: 0.9944761066827624\n",
            "Recall: 0.9961206957974366\n",
            "F1 Score: 0.9952763665917093\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "pred = model.predict(x_test1)\n",
        "predict_classes = np.argmax(pred,axis=1)\n",
        "expected_classes = np.argmax(y_test,axis=1)\n",
        "print(expected_classes.shape)\n",
        "print(predict_classes.shape)\n",
        "correct = accuracy_score(expected_classes,predict_classes)\n",
        "correct1 = precision_score(expected_classes,predict_classes,average='macro')\n",
        "correct2 = recall_score(expected_classes,predict_classes,average='macro')\n",
        "correct3 = f1_score(expected_classes,predict_classes,average='macro')\n",
        "print(f\"Training Accuracy: {correct}\")\n",
        "print(f\"Precision: {correct1}\")\n",
        "print(f\"Recall: {correct2}\")\n",
        "print(f\"F1 Score: {correct3}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJFchcIBi6zL"
      },
      "outputs": [],
      "source": [
        "# def prediction(dataframe):   \n",
        " \n",
        "#     # Making predictions \n",
        "    \n",
        "#     loaded_model = load_model('disease_types.h5')\n",
        "#     prediction1 = loaded_model.predict(dataframe)\n",
        "#     predict_classes = np.argmax(prediction1,axis=1)\n",
        "#     print(prediction1)\n",
        "#     print(predict_classes)\n",
        "#     prediction2 = statistics.mode(predict_classes)\n",
        "   \n",
        "     \n",
        "#     if (prediction2 == 0):\n",
        "#         pred = 'Dis1 Generalised Epilepsy'\n",
        "#     elif (prediction2 == 1):\n",
        "#         pred = 'Dis3 Generalised Epilepsy'\n",
        "#     elif (prediction2 == 2):\n",
        "#         pred = 'Delta Generalised Epilepsy'\n",
        "#     elif (prediction2 == 3):\n",
        "#         pred = 'Single burst Generalised Epilepsy'\n",
        "#     elif (prediction2 == 4):\n",
        "#         pred = 'Temporal Epilepsy'\n",
        "#     elif (prediction2 == 5):\n",
        "#         pred = 'Frontocentral Epilepsy'\n",
        "#     else:\n",
        "#         pred = 'Frontopolar Epilepsy'\n",
        "#     return pred\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5-LgP9Di62I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR32PZuci66y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDfRV3nvif4x"
      },
      "source": [
        "Early Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3SGx09KKzKs"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn\n",
        "!pip install eeglib\n",
        "!pip install mne\n",
        "!pip install pyEDFlib\n",
        "!pip install EDFlib-Python\n",
        "!pip install scikit-learn --pre\n",
        "!pip install eeglib\n",
        "import eeglib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FE04TtA8RPyO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef21e485-e4db-412b-c41b-87ac72db0b3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.11.0\n",
            "  Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.8.0)\n",
            "Collecting keras<2.12,>=2.11.0 (from tensorflow==2.11.0)\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (16.0.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (23.1)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.11.0)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Collecting tensorboard<2.12,>=2.11 (from tensorflow==2.11.0)\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0 (from tensorflow==2.11.0)\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.17.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.27.1)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard-data-server, protobuf, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.0\n",
            "    Uninstalling tensorboard-data-server-0.7.0:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.2\n",
            "    Uninstalling tensorboard-2.12.2:\n",
            "      Successfully uninstalled tensorboard-2.12.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.11.0 protobuf-3.19.6 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "!pip install tensorflow==2.11.0\n",
        "import tensorflow as tf\n",
        "# keras_clf = KerasClassifier(model = model, optimizer=tf.keras.optimizers.Adam(), epochs=100, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZT5Drt_FKzOR"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import pandas as pd\n",
        "\n",
        "from eeglib.helpers import EDFHelper\n",
        "import mne\n",
        "from collections import Counter\n",
        "import statistics\n",
        "# import streamlit as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import glob\n",
        "import librosa\n",
        "import numpy as np\n",
        "import librosa.display\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import loadtxt\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import GRU,LSTM\n",
        "from tensorflow.keras import layers\n",
        "tf.keras.backend.clear_session()\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import  ModelCheckpoint\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjtYMfADLO3z"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPhuvslnKzRZ"
      },
      "outputs": [],
      "source": [
        "epilepsy_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/final.csv')\n",
        "epilepsy_data =epilepsy_data.drop(['Unnamed: 0'],axis=1)\n",
        "epilepsy_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP2FFLUdKzU2"
      },
      "outputs": [],
      "source": [
        "\n",
        "x=epilepsy_data[['Mean','Std','Ptp','Var','Minim','Maxim','Argminim','Argmaxim','Mean_square','RMS','Abs_diffs_signal',\n",
        "                           'Skewness','Kurtosis','Spectral_centroid','Spectral_rolloff','Spectral_contrast','Spectral_bandwidth','Spectral_flatness','DWT','CorrelationDimension',\n",
        "                           'Zero_crossing_rate','SpectralEntropy','PFD']]\n",
        "y = epilepsy_data.loc[:,'Label'].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8ltkr9kKzXV"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x)\n",
        "x = scaler.transform(x)\n",
        "from keras.utils import to_categorical\n",
        "y = to_categorical(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biL5JOv2Lmp6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
        "x_train1 = np.reshape(x_train, (x_train.shape[0],1,x.shape[1]))\n",
        "x_test1 = np.reshape(x_test, (x_test.shape[0],1,x.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-T61aWiLoj_"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "# model.add(LSTM(128, input_shape=(1,18),activation=\"sigmoid\",return_sequences=True))\n",
        "# model.add(Dropout(0.3))\n",
        "model.add(LSTM(64, input_shape=(1,23),activation=\"relu\",return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(16,activation=\"sigmoid\"))\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(LSTM(100,return_sequences=True))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(LSTM(50))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(3, activation='sigmoid'))\n",
        "model.add(layers.Activation('softmax'))\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "from keras.optimizers import SGD\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
        "\n",
        "history = model.fit(x_train1, y_train, epochs = 100, validation_data= (x_test1, y_test))\n",
        "model.save('prediction.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT4jCnTPL0O3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = ['0', '1', '2']\n",
        "pred = model.predict(x_test1)\n",
        "predict_classes = np.argmax(pred,axis=1)\n",
        "expected_classes = np.argmax(y_test,axis=1)\n",
        "print(expected_classes.shape)\n",
        "print(predict_classes.shape)\n",
        "print(classification_report(expected_classes, predict_classes, target_names=target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlAKsXHJL0VP",
        "outputId": "cb7abc1f-e27a-45ef-e71b-699abc484a17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 0s 3ms/step\n",
            "(84,)\n",
            "(84,)\n",
            "Training Accuracy: 0.8809523809523809\n",
            "Precision: 0.875\n",
            "Recall: 0.8859180035650623\n",
            "F1 Score: 0.8784513993003499\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "pred = model.predict(x_test1)\n",
        "predict_classes = np.argmax(pred,axis=1)\n",
        "expected_classes = np.argmax(y_test,axis=1)\n",
        "print(expected_classes.shape)\n",
        "print(predict_classes.shape)\n",
        "correct = accuracy_score(expected_classes,predict_classes)\n",
        "correct1 = precision_score(expected_classes,predict_classes,average='macro')\n",
        "correct2 = recall_score(expected_classes,predict_classes,average='macro')\n",
        "correct3 = f1_score(expected_classes,predict_classes,average='macro')\n",
        "print(f\"Training Accuracy: {correct}\")\n",
        "print(f\"Precision: {correct1}\")\n",
        "print(f\"Recall: {correct2}\")\n",
        "print(f\"F1 Score: {correct3}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be2SHy61L-1P"
      },
      "outputs": [],
      "source": [
        "acc_train = history.history['loss']\n",
        "acc_val = history.history['val_loss']\n",
        "epochs = range(1,101)\n",
        "plt.plot(epochs, acc_train, 'g', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val, 'b', label='Validation accuracy')\n",
        "plt.title('Training and Validation accuracies')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.legend()\n",
        "#plt.show()\n",
        "plt.savefig(\"Accuracy_plot_LSTM.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-1z9XwUME9F"
      },
      "outputs": [],
      "source": [
        "loss_train = history.history['loss']\n",
        "loss_val = history.history['val_loss']\n",
        "epochs = range(1,101)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "plt.plot(epochs, loss_val, 'b', label='Validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "#plt.show()\n",
        "plt.savefig(\"Loss_plot_LSTM.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBS-QT2RO3nn"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Pan8F18MgQW",
        "outputId": "1559e25f-9cfe-4933-d49b-693a865f3f07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'pyeeg'...\n",
            "remote: Enumerating objects: 210, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 210 (delta 0), reused 0 (delta 0), pack-reused 207\u001b[K\n",
            "Receiving objects: 100% (210/210), 119.45 KiB | 2.02 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/forrestbao/pyeeg.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8Ok5cFQMzTm"
      },
      "outputs": [],
      "source": [
        "%cd pyeeg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za813M-AM249"
      },
      "outputs": [],
      "source": [
        "!python3 setup.py install\n",
        "!pip install nolds\n",
        "!pip install librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtwxubLfM3UR"
      },
      "outputs": [],
      "source": [
        "import pyeeg\n",
        "import nolds\n",
        "import pywt\n",
        "import math\n",
        "import librosa\n",
        "\n",
        "def PFD( x ):\n",
        "\tresp = pyeeg.pfd(x)\n",
        "\treturn resp\n",
        "\n",
        "def CorrelationDimension( x ):\n",
        "\tresp = nolds.corr_dim(x,1)\n",
        "\treturn resp\n",
        "\n",
        "def DWT( x ):\n",
        "\tresp = pywt.dwt(x, 'db4')\n",
        "\treturn resp\n",
        "\n",
        "\n",
        "def SpectralEntropy( x ):\n",
        "\tfs = 128\n",
        "\tband = [1,4,8,12,30]\n",
        "\tb = pyeeg.bin_power(x,band,fs)\n",
        "\tresp = pyeeg.spectral_entropy(x,band,fs,Power_Ratio=b)\n",
        "\tresp = [0 if math.isnan(x) else x for x in resp]\n",
        "\treturn resp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCnzsdrvM3Xp"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "def mean(data):\n",
        "    return np.mean(data,axis=-1)\n",
        "    \n",
        "def std(data):\n",
        "    return np.std(data,axis=-1)\n",
        "\n",
        "def ptp(data):\n",
        "    return np.ptp(data,axis=-1)\n",
        "\n",
        "def var(data):\n",
        "        return np.var(data,axis=-1)\n",
        "\n",
        "def minim(data):\n",
        "      return np.min(data,axis=-1)\n",
        "\n",
        "\n",
        "def maxim(data):\n",
        "      return np.max(data,axis=-1)\n",
        "\n",
        "def argminim(data):\n",
        "      return np.argmin(data,axis=-1)\n",
        "\n",
        "\n",
        "def argmaxim(data):\n",
        "      return np.argmax(data,axis=-1)\n",
        "\n",
        "def mean_square(data):\n",
        "      return np.mean(data**2,axis=-1)\n",
        "\n",
        "def rms(data): #root mean square\n",
        "      return  np.sqrt(np.mean(data**2,axis=-1))  \n",
        "\n",
        "def abs_diffs_signal(data):\n",
        "    return np.sum(np.abs(np.diff(data,axis=-1)),axis=-1)\n",
        "\n",
        "\n",
        "def skewness(data):\n",
        "    return stats.skew(data,axis=-1)\n",
        "\n",
        "def kurtosis(data):\n",
        "    return stats.kurtosis(data,axis=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9uEgvEHM3a5",
        "outputId": "e32c88c5-5c14-424f-e062-b93ca16ad200"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "869ZZ-zrNJop"
      },
      "outputs": [],
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import glob\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "mat1 = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/EEG Epilepsy Datasets/Test/interictal/interictal40.mat')\n",
        "value1 = mat1['interictal']\n",
        "data1 = np.array(value1)\n",
        "mat2 = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/EEG Epilepsy Datasets/Test/ictal/ictal45.mat')\n",
        "value2 = mat2['ictal']\n",
        "data2 = np.array(value2)\n",
        "mat3 = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/EEG Epilepsy Datasets/Test/preictal/preictal40.mat')\n",
        "value3 = mat3['preictal']\n",
        "data3 = np.array(value3)\n",
        "mat4 = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/EEG Epilepsy Datasets/Test/interictal/interictal38.mat')\n",
        "value4 = mat4['interictal']\n",
        "data4 = np.array(value4)\n",
        "mat5 = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/EEG Epilepsy Datasets/Test/preictal/preictal45.mat')\n",
        "value5 = mat5['preictal']\n",
        "data5 = np.array(value5)\n",
        "mat6 = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/EEG Epilepsy Datasets/Test/ictal/ictal50.mat')\n",
        "value6 = mat6['ictal']\n",
        "data6 = np.array(value6)\n",
        "list1 = [data1,data2,data3,data4,data5,data6]\n",
        "signal = np.concatenate([np.expand_dims(i,axis=0) for i in list1 ])\n",
        "result = signal.ravel()\n",
        "print(result)\n",
        "result.shape\n",
        "\n",
        "\n",
        "df = pd.DataFrame(columns=['Mean','Std','Ptp','Var','Minim','Maxim','Argminim','Argmaxim','Mean_square','RMS','Abs_diffs_signal',\n",
        "                           'Skewness','Kurtosis','Spectral_centroid','Spectral_rolloff','Spectral_contrast','Spectral_bandwidth','Spectral_flatness','DWT','CorrelationDimension',\n",
        "                           'Zero_crossing_rate','SpectralEntropy','PFD'])\n",
        "result=result.flatten()\n",
        "a=0\n",
        "b=1024\n",
        "while b<=6144:\n",
        "     o=[]\n",
        "     for i in range(a,b):\n",
        "       o.append(result[i])\n",
        "     o1=np.array(o)\n",
        "     convertedArray = o1.astype(np.float)\n",
        "     df2={'Mean':mean(o1),'Std':std(o1),'Ptp':ptp(o1),'Var':var(o1),'Minim':minim(o1),'Maxim':maxim(o1),'Argminim':argminim(o1),'Argmaxim':argmaxim(o1),\n",
        "       'Mean_square':mean_square(o1),'RMS':rms(o1),'Abs_diffs_signal':abs_diffs_signal(o1),'Skewness':skewness(o1),'Kurtosis':kurtosis(o1),'Spectral_centroid':librosa.feature.spectral_centroid(y=convertedArray)[0][0],\n",
        "       'Spectral_rolloff':librosa.feature.spectral_rolloff(y=convertedArray)[0][0],'Spectral_contrast':librosa.feature.spectral_contrast(y=convertedArray)[0][0],\n",
        "       'Spectral_bandwidth':librosa.feature.spectral_bandwidth(y=convertedArray)[0][0],'Spectral_flatness':librosa.feature.spectral_flatness(y=convertedArray)[0][0],\n",
        "       'DWT':np.mean(DWT(o1),axis=-1)[0],'CorrelationDimension':CorrelationDimension(o1),\n",
        "     'Zero_crossing_rate':librosa.feature.zero_crossing_rate(y=convertedArray)[0][0],'SpectralEntropy':np.mean(SpectralEntropy(o1),axis=-1),'PFD':PFD(o1)}\n",
        "     df.loc[len(df)] = df2\n",
        "     a+=1024\n",
        "     b+=1024\n",
        "   #df = df.append(df2, ignore_index = True)\n",
        "df.to_csv('preictal.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue0ByJ_ONgYV",
        "outputId": "174d0bae-43b7-4319-bb9e-c1d623b7dd89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dataframe=df[['Mean','Std','Ptp','Var','Minim','Maxim','Argminim','Argmaxim','Mean_square','RMS','Abs_diffs_signal',\n",
        "                           'Skewness','Kurtosis','Spectral_centroid','Spectral_rolloff','Spectral_contrast','Spectral_bandwidth','Spectral_flatness','DWT','CorrelationDimension'\n",
        "            ,'Zero_crossing_rate','SpectralEntropy','PFD']]\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(dataframe)\n",
        "dataframe = scaler.transform(dataframe)\n",
        "dataframe1 = np.reshape(dataframe, (dataframe.shape[0],1,dataframe.shape[1]))\n",
        "dataframe1.shape\n",
        "rr = model.predict(dataframe1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2hPEoGeNVYX",
        "outputId": "4c5cc584-6439-4eec-d67c-f5013ef5376e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted:  [1 2 0 1 1 2]\n"
          ]
        }
      ],
      "source": [
        "# print('Original:  ',np.array([0,0,0,0,2,2,2,2,2,2,2,2,0,0,0,0,2,2,2,2,1,1,1,1]))\n",
        "expected_classes = np.argmax(rr,axis=1)\n",
        "print('Predicted: ',expected_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwgwvxfmQLSz"
      },
      "source": [
        "Predictive system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvsFHaUcQOGT"
      },
      "outputs": [],
      "source": [
        "input_data = (1.022224036,1.027655534,1.026371185,1.026371185,1.026628492,1.026371185,1.026371185,1.024563855,1.027142449,1.026113658,1.026371185,1.026628492,1.023266214,1.026628492,1.02688558,1.026628492,1.031223035,1.023526192,1.025339754,1.02482271,1.02688558,1.027399101,1.020916175,144.26233,0.784223518,0.447766546,0.694166247,0.231925094,0.736146972,0.809544556,0.75158713,0.944293304,0.513480739,0.908025389,0.737497691,0.263564365,0.216937721,0.856380414,0.411747359,4.848634104,0.080859415,1.052120272,0.461157591,0.685372056,0.676203895,0.558924926,1.172187715)\n",
        "\n",
        "input_data_as_numpy_array = np.asarray(input_data)\n",
        "input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n",
        "prediction = model.predict(input_data_reshaped)\n",
        "print(prediction)\n",
        "\n",
        "\n",
        "if (prediction == 0):\n",
        "  print(\"dis1 generalized\")\n",
        "elif (prediction == 1):\n",
        "  print(\"dis3 generalized\")\n",
        "elif (prediction == 2):\n",
        "  print(\" delta generalized\")\n",
        "elif (prediction == 3):\n",
        "  print(\"single burst generalized\")\n",
        "elif (prediction == 4):\n",
        "  print(\"temporal\")\n",
        "elif (prediction == 5):\n",
        "  print(\"frontocentral\")\n",
        "elif (prediction == 6):\n",
        "  print(\"frontopolar\")\n",
        "else:\n",
        "  print(\"The Person doesnot have epilepsy\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kaiOBbSO7jF"
      },
      "source": [
        "Model Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1lwQicdPw3W"
      },
      "outputs": [],
      "source": [
        "# import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBvLbmXDPzYw"
      },
      "outputs": [],
      "source": [
        "# filename = 'epilepsy_early.sav'\n",
        "# pickle.dump(model, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1eaNdChO9gp"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# filename = 'epilepsy_early.sav'\n",
        "# pickle.dump(model, open(filename, 'wb'))\n",
        "# loading the saved model\n",
        "# epilepsy_early = pickle.load(open('epilepsy_early.sav', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji_4QGVJ4Ysg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# %%writefile app.py\n",
        "from __future__ import print_function\n",
        "# -*- coding: utf-8 -*-\n",
        "import pickle\n",
        "import streamlit as st\n",
        "from streamlit_option_menu import option_menu\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import eeglib\n",
        "from eeglib.helpers import EDFHelper\n",
        "import mne\n",
        "from collections import Counter\n",
        "import statistics\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.pagesizes import letter\n",
        "import time\n",
        "import random\n",
        "import glob\n",
        "import librosa\n",
        "import numpy as np\n",
        "import librosa.display\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import loadtxt\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import  ModelCheckpoint\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "from sklearn.metrics import classification_report\n",
        "import h5py\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "\n",
        "# loading the saved models\n",
        "\n",
        "#epilepsy_file = load_model('/content/epilepsy_types.h5', 'r')\n",
        "#disease_file = load_model('/content/disease_types.h5', 'r')\n",
        "#prediction_model\n",
        "#epilepsy_model = h5py.File(open('/content/epilepsy_types.h5', 'rb'))\n",
        "#disease_model = pickle.load(open('/content/epilepsy_model.sav', 'rb')) \n",
        "#prediction_model = pickle.load(open('/content/epilepsy_model.sav', 'rb'))  \n",
        "\n",
        "st.cache_data()\n",
        "\n",
        "def generate_pdf_report(variables):\n",
        "    # Create a new PDF file\n",
        "    pdf_file = canvas.Canvas(\"Report.pdf\", pagesize=letter)\n",
        "\n",
        "    # Set the title of the report\n",
        "    report_title = \"Report\"\n",
        "\n",
        "    # Add the report title to the PDF file\n",
        "    pdf_file.setFont(\"Helvetica-Bold\", 16)\n",
        "    pdf_file.drawCentredString(300, 750, report_title)\n",
        "    pdf_file.line(30, 740, 550, 740)\n",
        "\n",
        "    # Add the variables to the PDF file\n",
        "    pdf_file.setFont(\"Helvetica\", 12)\n",
        "    y = 700\n",
        "    for name, value in variables.items():\n",
        "        pdf_file.drawString(50, y, f\"{name}: {value}\")\n",
        "        y -= 20\n",
        "\n",
        "    # Save the PDF file\n",
        "    pdf_file.save()\n",
        "    return os.path.abspath(\"Report.pdf\") \n",
        "\n",
        "\n",
        "# sidebar for navigation\n",
        "with st.sidebar:\n",
        "    \n",
        "    selected = option_menu('Multiple Disease Prediction System',\n",
        "                          \n",
        "                          ['Epilepsy Classification',\n",
        "                            'Disease Classification',\n",
        "                           'Early Detection',],\n",
        "                          icons=['activity','heart','person'],\n",
        "                          default_index=0)\n",
        "\n",
        "\n",
        "# Epilepsy Classification\n",
        "if (selected == \"Epilepsy Classification\"):\n",
        "    \n",
        "    # page title\n",
        "    st.title(\"Epilepsy Classification using Deep Learning\")\n",
        "    \n",
        "    Patient_id =st.text_input(\"Enter your Patient Id\")\n",
        "    gender = st.selectbox(\"Enter your gender\",[\"Male\", \"Female\"])\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    age = col2.number_input(\"Enter your age\")\n",
        "    stroke = col1.selectbox(\"Have you ever experienced a seizure?\",[\"Yes\",\"No\"])\n",
        "    uploaded_file = st.file_uploader(\"Upload the eeg (EDF)\")\n",
        "    if uploaded_file is not None :\n",
        "      with open(os.path.join(\"/content/uploaded_file\",\"patient.edf\"),\"wb\") as f: \n",
        "            f.write(uploaded_file.getbuffer())       \n",
        "    \n",
        "    \n",
        "    # code for Prediction\n",
        "    def prediction(dataframe):   \n",
        " \n",
        "    # Making predictions \n",
        "        loaded_model = load_model('/content/epilepsy_types.h5')\n",
        "        prediction1 = loaded_model.predict(dataframe)\n",
        "        predict_classes = np.argmax(prediction1,axis=1)\n",
        "        print(prediction1)\n",
        "        print(predict_classes)\n",
        "        prediction2 = statistics.mode(predict_classes)\n",
        "      \n",
        "        \n",
        "        if (prediction2 == 0):\n",
        "            pred = 'Dis1 Generalised Epilepsy'\n",
        "        elif (prediction2 == 1):\n",
        "            pred = 'Dis3 Generalised Epilepsy'\n",
        "        elif (prediction2 == 2):\n",
        "            pred = 'Delta Generalised Epilepsy'\n",
        "        elif (prediction2 == 3):\n",
        "            pred = 'Single burst Generalised Epilepsy'\n",
        "        elif (prediction2 == 4):\n",
        "            pred = 'Temporal Epilepsy'\n",
        "        elif (prediction2 == 5):\n",
        "            pred = 'Frontocentral Epilepsy'\n",
        "        else:\n",
        "            pred = 'Frontopolar Epilepsy'\n",
        "        return pred\n",
        "    \n",
        "    # creating a button for Prediction\n",
        "    \n",
        "    if st.button(\"Predict\"):\n",
        "      st.write(\"Processing Please Wait\") \n",
        "      s1= mne.io.read_raw_edf('/content/uploaded_file/patient.edf',include=['C3',\n",
        "      'C4',\n",
        "      'Cz',\n",
        "      'F3',\n",
        "      'F4',\n",
        "      'F7',\n",
        "      'F8',\n",
        "      'Fz',\n",
        "      'Fp1',\n",
        "      'Fp2',\n",
        "      'A1',\n",
        "      'A2',\n",
        "      'O1',\n",
        "      'O2',\n",
        "      'P3',\n",
        "      'P4',\n",
        "      'T5',\n",
        "      'T6',\n",
        "      'Pz',\n",
        "      'T3',\n",
        "      'T4',\n",
        "      'ECG-LA',\n",
        "      'ECG-RA'])\n",
        "      s2 = mne.export.export_raw('/content/patient1.edf',s1,overwrite=True)\n",
        "      helper= EDFHelper(\"/content/patient1.edf\",lowpass=70, highpass=1, normalize=True, ICA=True,windowSize = 20)\n",
        "      wrapper=eeglib.wrapper.Wrapper(helper)\n",
        "      wrapper.addFeature.PFD()\n",
        "      wrapper.addFeature.DTW((0,1))\n",
        "      wrapper.addFeature.hjorthActivity()\n",
        "      data=wrapper.getAllFeatures()\n",
        "      data.to_csv('patient.csv')\n",
        "      dataframe1 = pd.read_csv('patient.csv')\n",
        "      dataframe1 = dataframe1.drop(dataframe1.columns[0],axis=1)\n",
        "      dataframe1.columns = ['PFD0', 'PFD1', 'PFD2', 'PFD3', 'PFD4', 'PFD5', 'PFD6', 'PFD7', 'PFD8',\n",
        "    'PFD9', 'PFD10', 'PFD11', 'PFD12', 'PFD13', 'PFD14', 'PFD15', 'PFD16',\n",
        "    'PFD17', 'PFD18', 'PFD19', 'PFD20', 'PFD21', 'PFD22', 'DTW',\n",
        "    'hjorthActivity0', 'hjorthActivity1', 'hjorthActivity2',\n",
        "    'hjorthActivity3', 'hjorthActivity4', 'hjorthActivity5',\n",
        "    'hjorthActivity6', 'hjorthActivity7', 'hjorthActivity8',\n",
        "    'hjorthActivity9', 'hjorthActivity10', 'hjorthActivity11',\n",
        "    'hjorthActivity12', 'hjorthActivity13', 'hjorthActivity14',\n",
        "    'hjorthActivity15', 'hjorthActivity16', 'hjorthActivity17',\n",
        "    'hjorthActivity18', 'hjorthActivity19', 'hjorthActivity20',\n",
        "    'hjorthActivity21', 'hjorthActivity22']\n",
        "      dataframe1.to_csv(\"patient_app.csv\")\n",
        "      dataframe3 = pd.read_csv('patient_app.csv')\n",
        "      dataframe2 = dataframe3.drop(dataframe3.columns[0],axis=1)\n",
        "      dataframe2 = scaler.transform(dataframe2)\n",
        "      dataframe = np.array(dataframe1.values).reshape((dataframe1.shape[0],1,47))\n",
        "      result = prediction(dataframe) \n",
        "      variables = {\"Name\": Patient_id, \"Age\": age, \"Gender\":gender, \"Have you ever experienced a seizure\":stroke, \"Result\":result} \n",
        "\n",
        "              # Display the variables in Streamlit\n",
        "              # Generate the PDF report\n",
        "      pdf_file_path = generate_pdf_report(variables)\n",
        "      with st.expander(\"Download PDF Report\"):\n",
        "              st.write(\"Click the button below to download the PDF report\")\n",
        "              st.download_button(\n",
        "                  label=\"Download Report\",\n",
        "                  data=open(pdf_file_path, \"rb\").read(),\n",
        "                  file_name=\"streamlit_variables.pdf\",\n",
        "                  mime=\"application/pdf\"\n",
        "              )  \n",
        "      st.success('Patient disease is {}'.format(result))\n",
        "      print(result)\n",
        "    \n",
        "\n",
        "\n",
        "# Diabetes Prediction Page\n",
        "if (selected == 'Disease Classification'):\n",
        "    \n",
        "    # page title\n",
        "    st.title('Diabetes Prediction using ML')\n",
        "    \n",
        "    \n",
        "    Patient_id =st.text_input(\"Enter your Patient Id\")\n",
        "    gender = st.selectbox(\"Enter your gender\",[\"Male\", \"Female\"])\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    age = col2.number_input(\"Enter your age\")\n",
        "    stroke = col1.selectbox(\"Have you ever experienced a seizure?\",[\"Yes\",\"No\"])\n",
        "    uploaded_file = st.file_uploader(\"Upload the eeg (EDF)\")\n",
        "    if uploaded_file is not None :\n",
        "      with open(os.path.join(\"/content/uploaded_file\",\"patient.edf\"),\"wb\") as f: \n",
        "            f.write(uploaded_file.getbuffer())       \n",
        "    \n",
        "    \n",
        "    # code for Prediction\n",
        "    def prediction(dataframe):   \n",
        " \n",
        "    # Making predictions \n",
        "        loaded_model = load_model('/content/disease_types.h5')\n",
        "        prediction3 = loaded_model.predict(dataframe)\n",
        "        predict_classes = np.argmax(prediction1,axis=1)\n",
        "        print(prediction3)\n",
        "        print(predict_classes)\n",
        "        prediction4 = statistics.mode(predict_classes)\n",
        "      \n",
        "        \n",
        "        if (prediction4 == 0):\n",
        "            pred = 'Epileptic Patient'\n",
        "        elif (prediction4 == 1):\n",
        "            pred = 'Normal Patient'\n",
        "        elif (prediction4 == 2):\n",
        "            pred = 'intracerebral hemorrhage Patient'\n",
        "        elif (prediction4 == 3):\n",
        "            pred = 'Alzheimeric Patient'\n",
        "        elif (prediction4 == 4):\n",
        "            pred = 'insomniac Patient'\n",
        "        return pred\n",
        "    \n",
        "    # creating a button for Prediction\n",
        "    \n",
        "    if st.button(\"Predict\"):\n",
        "      st.write(\"Processing Please Wait\") \n",
        "      s1= mne.io.read_raw_edf('/content/uploaded_file/patient.edf',include=['C3',\n",
        "      'C4',\n",
        "      'Cz',\n",
        "      'F3',\n",
        "      'F4',\n",
        "      'F7',\n",
        "      'F8',\n",
        "      'Fz',\n",
        "      'Fp1',\n",
        "      'Fp2',\n",
        "      'A1',\n",
        "      'A2',\n",
        "      'O1',\n",
        "      'O2',\n",
        "      'P3',\n",
        "      'P4',\n",
        "      'T5',\n",
        "      'T6',\n",
        "      'Pz',\n",
        "      'T3',\n",
        "      'T4',\n",
        "      'ECG-LA',\n",
        "      'ECG-RA'])\n",
        "      s2 = mne.export.export_raw('/content/patient1.edf',s1,overwrite=True)\n",
        "      helper= EDFHelper(\"/content/patient1.edf\",lowpass=70, highpass=1, normalize=True, ICA=True,windowSize = 20)\n",
        "      wrapper=eeglib.wrapper.Wrapper(helper)\n",
        "      wrapper.addFeature.PFD()\n",
        "      wrapper.addFeature.DTW((0,1))\n",
        "      wrapper.addFeature.hjorthActivity()\n",
        "      data=wrapper.getAllFeatures()\n",
        "      data.to_csv('patient.csv')\n",
        "      dataframe1 = pd.read_csv('patient.csv')\n",
        "      dataframe1 = dataframe1.drop(dataframe1.columns[0],axis=1)\n",
        "      dataframe1.columns = ['PFD0', 'PFD1', 'PFD2', 'PFD3', 'PFD4', 'PFD5', 'PFD6', 'PFD7', 'PFD8',\n",
        "    'PFD9', 'PFD10', 'PFD11', 'PFD12', 'PFD13', 'PFD14', 'PFD15', 'PFD16',\n",
        "    'PFD17', 'PFD18', 'PFD19', 'PFD20', 'PFD21', 'PFD22', 'DTW',\n",
        "    'hjorthActivity0', 'hjorthActivity1', 'hjorthActivity2',\n",
        "    'hjorthActivity3', 'hjorthActivity4', 'hjorthActivity5',\n",
        "    'hjorthActivity6', 'hjorthActivity7', 'hjorthActivity8',\n",
        "    'hjorthActivity9', 'hjorthActivity10', 'hjorthActivity11',\n",
        "    'hjorthActivity12', 'hjorthActivity13', 'hjorthActivity14',\n",
        "    'hjorthActivity15', 'hjorthActivity16', 'hjorthActivity17',\n",
        "    'hjorthActivity18', 'hjorthActivity19', 'hjorthActivity20',\n",
        "    'hjorthActivity21', 'hjorthActivity22']\n",
        "      dataframe1.to_csv(\"patient_app.csv\")\n",
        "      dataframe3 = pd.read_csv('patient_app.csv')\n",
        "      dataframe2 = dataframe3.drop(dataframe3.columns[0],axis=1)\n",
        "      dataframe2 = scaler.transform(dataframe2)\n",
        "      dataframe = np.array(dataframe1.values).reshape((dataframe1.shape[0],1,47))\n",
        "      result = prediction(dataframe) \n",
        "      variables = {\"Name\": Patient_id, \"Age\": age, \"Gender\":gender, \"Have you ever experienced a seizure\":stroke, \"Result\":result} \n",
        "\n",
        "              # Display the variables in Streamlit\n",
        "              # Generate the PDF report\n",
        "      pdf_file_path = generate_pdf_report(variables)\n",
        "      with st.expander(\"Download PDF Report\"):\n",
        "              st.write(\"Click the button below to download the PDF report\")\n",
        "              st.download_button(\n",
        "                  label=\"Download Report\",\n",
        "                  data=open(pdf_file_path, \"rb\").read(),\n",
        "                  file_name=\"streamlit_variables.pdf\",\n",
        "                  mime=\"application/pdf\"\n",
        "              )  \n",
        "      st.success('Patient disease is {}'.format(result))\n",
        "      print(result)\n",
        "\n",
        "\n",
        "\n",
        "# Early Detection Page\n",
        "if (selected == 'Early Detection'):\n",
        "    \n",
        "    # page title\n",
        "    st.title('Early Detection using EEG')\n",
        "    \n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    \n",
        "    with col1:\n",
        "        age = st.text_input('Age')\n",
        "        \n",
        "    with col2:\n",
        "        sex = st.text_input('Sex')\n",
        "        \n",
        "    with col3:\n",
        "        cp = st.text_input('Chest Pain types')\n",
        "        \n",
        "    with col1:\n",
        "        trestbps = st.text_input('Resting Blood Pressure')\n",
        "        \n",
        "    with col2:\n",
        "        chol = st.text_input('Serum Cholestoral in mg/dl')\n",
        "        \n",
        "    with col3:\n",
        "        fbs = st.text_input('Fasting Blood Sugar > 120 mg/dl')\n",
        "        \n",
        "    with col1:\n",
        "        restecg = st.text_input('Resting Electrocardiographic results')\n",
        "        \n",
        "    with col2:\n",
        "        thalach = st.text_input('Maximum Heart Rate achieved')\n",
        "        \n",
        "    with col3:\n",
        "        exang = st.text_input('Exercise Induced Angina')\n",
        "        \n",
        "    with col1:\n",
        "        oldpeak = st.text_input('ST depression induced by exercise')\n",
        "        \n",
        "    with col2:\n",
        "        slope = st.text_input('Slope of the peak exercise ST segment')\n",
        "        \n",
        "    with col3:\n",
        "        ca = st.text_input('Major vessels colored by flourosopy')\n",
        "        \n",
        "    with col1:\n",
        "        thal = st.text_input('thal: 0 = normal; 1 = fixed defect; 2 = reversable defect')\n",
        "        \n",
        "    # code for Prediction\n",
        "    early_detection = ''\n",
        "    \n",
        "    # creating a button for Prediction    \n",
        "    if st.button(\"Epilepsy Classification\"):\n",
        "        epilepsy_prediction = detection_model.predict([[PFD0,PFD1,PFD2,PFD3,PFD4,PFD5,PFD6,PFD7,PFD8,PFD9,PFD10,PFD11,PFD12,PFD13,PFD14,PFD15,PFD16,PFD17,PFD18,PFD19,PFD20,PFD21,PFD22,DTW,hjorthActivity0,hjorthActivity1,hjorthActivity2,hjorthActivity3,hjorthActivity4,hjorthActivity5,hjorthActivity6,hjorthActivity7,hjorthActivity8,hjorthActivity9,hjorthActivity10,hjorthActivity11,hjorthActivity12,hjorthActivity13,hjorthActivity14,hjorthActivity15,hjorthActivity16,hjorthActivity17,hjorthActivity18,hjorthActivity19,hjorthActivity20,hjorthActivity21,hjorthActivity22]])                          \n",
        "        \n",
        "\n",
        "        if (epilepsy_prediction[0] == 0):\n",
        "          early_detection = \"Preictal\"\n",
        "        elif (epilepsy_prediction[0] == 1):\n",
        "          early_detection = \"Interictal\"\n",
        "        elif (epilepsy_prediction[0] == 2):\n",
        "          early_detection = \"Ictal\"\n",
        "        else:\n",
        "          early_detection = \"No stage Found\"\n",
        "        \n",
        "    st.success(early_detection)        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETZzHMAkCW8J"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "\n",
        "def compress_edf(input_file, output_file):\n",
        "    with open(input_file, 'rb') as f_in:\n",
        "        with gzip.open(output_file, 'wb') as f_out:\n",
        "            f_out.writelines(f_in)\n",
        "\n",
        "input_file = '/content/ABDULSALMAN~ A_c6227b77-05c9-453a-a5cf-f9bd1fced1e3.edf'\n",
        "output_file = 'compressed.edf'\n",
        "\n",
        "compress_edf(input_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTZeFNKROwg0"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "\n",
        "def compress_edf(input_file, output_file, target_size):\n",
        "    with open(input_file, 'rb') as f_in:\n",
        "        with gzip.open(output_file, 'wb') as f_out:\n",
        "            f_out.writelines(f_in)\n",
        "\n",
        "# Usage example\n",
        "input_file = '/content/ABDULSALMAN~ A_c6227b77-05c9-453a-a5cf-f9bd1fced1e3.edf'\n",
        "output_file = 'compressednew.edf'\n",
        "target_size = 30000000  # Specify the desired size in bytes\n",
        "compress_edf(input_file, output_file, target_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZo8BCnY13Yv",
        "outputId": "1c209ebf-5520-460b-a7e9-3f5758b8097e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app1.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile app1.py\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "# -*- coding: utf-8 -*-\n",
        "import pickle\n",
        "import streamlit as st\n",
        "from streamlit_option_menu import option_menu\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import eeglib\n",
        "from eeglib.helpers import EDFHelper\n",
        "import mne\n",
        "from collections import Counter\n",
        "import statistics\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.pagesizes import letter\n",
        "import time\n",
        "import random\n",
        "import glob\n",
        "import librosa\n",
        "import numpy as np\n",
        "import librosa.display\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import loadtxt\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import  ModelCheckpoint\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "from sklearn.metrics import classification_report\n",
        "import h5py\n",
        "from keras.models import load_model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "\n",
        "# loading the saved models\n",
        "\n",
        "#epilepsy_file = load_model('/content/epilepsy_types.h5', 'r')\n",
        "#disease_file = load_model('/content/disease_types.h5', 'r')\n",
        "#prediction_model\n",
        "#epilepsy_model = h5py.File(open('/content/epilepsy_types.h5', 'rb'))\n",
        "#disease_model = pickle.load(open('/content/epilepsy_model.sav', 'rb')) \n",
        "#prediction_model = pickle.load(open('/content/epilepsy_model.sav', 'rb'))  \n",
        "\n",
        "def generate_pdf_report(variables):\n",
        "   \n",
        "    pdf_file = canvas.Canvas(\"Report.pdf\", pagesize=letter)\n",
        "    report_title = \"Report\"\n",
        "    pdf_file.setFont(\"Helvetica-Bold\", 16)\n",
        "    pdf_file.drawCentredString(300, 750, report_title)\n",
        "    pdf_file.line(30, 740, 550, 740)\n",
        "    pdf_file.setFont(\"Helvetica\", 12)\n",
        "    y = 700\n",
        "    for name, value in variables.items():\n",
        "        pdf_file.drawString(50, y, f\"{name}: {value}\")\n",
        "        y -= 20\n",
        "    pdf_file.save()\n",
        "    return os.path.abspath(\"Report.pdf\") \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# sidebar for navigation\n",
        "with st.sidebar:\n",
        "    \n",
        "    selected = option_menu('Multiple Disease Prediction System',\n",
        "                          \n",
        "                          ['Epilepsy Classification',\n",
        "                            'Disease Classification',\n",
        "                           'Early Detection',],\n",
        "                          icons=['activity','heart','person'],\n",
        "                          default_index=0)\n",
        "\n",
        "\n",
        "# Epilepsy Classification\n",
        "if (selected == \"Epilepsy Classification\"):\n",
        "    \n",
        "    # page title\n",
        "    st.title(\"Epilepsy Classification using Deep Learning\")\n",
        "    \n",
        "    Patient_id =st.text_input(\"Enter your Patient Id\")\n",
        "    gender = st.selectbox(\"Enter your gender\",[\"Male\", \"Female\"])\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    age = col2.number_input(\"Enter your age\")\n",
        "    stroke = col1.selectbox(\"Have you ever experienced a seizure?\",[\"Yes\",\"No\"])\n",
        "    disease_sample = st.file_uploader(\"Upload the eeg (EDF)\")\n",
        "    if disease_sample is not None :\n",
        "      with open(os.path.join(\"/content/disease_sample\",\"patient.edf\"),\"wb\") as f: \n",
        "            f.write(disease_sample.getbuffer())      \n",
        "    \n",
        "    \n",
        "    # code for Prediction\n",
        "    def prediction(dataframe):   \n",
        "        loaded_model = load_model('/content/epilepsy_types.h5')\n",
        "        prediction1 = loaded_model.predict(dataframe)\n",
        "        predict_classes = np.argmax(prediction1,axis=1)\n",
        "        prediction2 = statistics.mode(predict_classes)\n",
        "        print(prediction2)\n",
        "        if (prediction2 == 0):\n",
        "            pred = 'Dis1 Generalised Epilepsy'\n",
        "        elif (prediction2 == 1):\n",
        "            pred = 'Dis3 Generalised Epilepsy'\n",
        "        elif (prediction2 == 2):\n",
        "            pred = 'Delta Generalised Epilepsy'\n",
        "        elif (prediction2 == 3):\n",
        "            pred = 'Single burst Generalised Epilepsy'\n",
        "        elif (prediction2 == 4):\n",
        "            pred = 'Temporal Epilepsy'\n",
        "        elif (prediction2 == 5):\n",
        "            pred = 'Frontocentral Epilepsy'\n",
        "        else:\n",
        "            pred = 'Frontopolar Epilepsy'\n",
        "        return pred\n",
        "\n",
        "    if st.button(\"Predict\"):\n",
        "      st.write(\"Processing Please Wait\") \n",
        "      s1= mne.io.read_raw_edf('/content/disease_sample/patient.edf',include=['C3','C4','Cz','F3','F4','F7','F8','Fz','Fp1','Fp2','A1','A2','O1','O2',\n",
        "      'P3','P4','T5','T6','Pz','T3','T4','ECG-LA','ECG-RA'])\n",
        "      s2 = mne.export.export_raw('/content/patient1.edf',s1,overwrite=True)\n",
        "      helper= EDFHelper('/content/patient1.edf',lowpass=70, highpass=1, normalize=True, ICA=True,windowSize = 20)\n",
        "      wrapper=eeglib.wrapper.Wrapper(helper)\n",
        "      wrapper.addFeature.PFD()\n",
        "      wrapper.addFeature.DTW((0,1))\n",
        "      wrapper.addFeature.hjorthActivity()\n",
        "      data=wrapper.getAllFeatures()\n",
        "      data.to_csv('patient.csv')\n",
        "      dataframe1 = pd.read_csv('patient.csv')\n",
        "      dataframe1 = dataframe1.drop(dataframe1.columns[0],axis=1)\n",
        "      dataframe1.columns = ['PFD0', 'PFD1', 'PFD2', 'PFD3', 'PFD4', 'PFD5', 'PFD6', 'PFD7', 'PFD8','PFD9', 'PFD10', 'PFD11', 'PFD12', 'PFD13', 'PFD14', 'PFD15', 'PFD16',\n",
        "                           'PFD17', 'PFD18', 'PFD19', 'PFD20', 'PFD21', 'PFD22', 'DTW','hjorthActivity0', 'hjorthActivity1', 'hjorthActivity2',\n",
        "                           'hjorthActivity3', 'hjorthActivity4', 'hjorthActivity5','hjorthActivity6', 'hjorthActivity7', 'hjorthActivity8',\n",
        "                           'hjorthActivity9', 'hjorthActivity10', 'hjorthActivity11','hjorthActivity12', 'hjorthActivity13', 'hjorthActivity14',\n",
        "                           'hjorthActivity15', 'hjorthActivity16', 'hjorthActivity17','hjorthActivity18', 'hjorthActivity19', 'hjorthActivity20',\n",
        "                            'hjorthActivity21', 'hjorthActivity22']\n",
        "      dataframe1.to_csv(\"patient_app.csv\")\n",
        "      dataframe3 = pd.read_csv('patient_app.csv')\n",
        "      dataframe2 = dataframe3.drop(dataframe3.columns[0],axis=1)\n",
        "      scaler.fit(dataframe2)\n",
        "      dataframe2 = scaler.transform(dataframe2)\n",
        "      dataframe = np.array(dataframe2).reshape((dataframe2.shape[0],1,47))\n",
        "      result = prediction(dataframe) \n",
        "      variables = {\"Name\": Patient_id, \"Age\": age, \"Gender\":gender, \"Have you ever experienced a seizure\":stroke, \"Result\":result}      \n",
        "      pdf_file_path = generate_pdf_report(variables)\n",
        "      with st.expander(\"Download PDF Report\"):\n",
        "              st.write(\"Click the button below to download the PDF report\")\n",
        "              st.download_button(\n",
        "                  label=\"Download Report\",\n",
        "                  data=open(pdf_file_path, \"rb\").read(),\n",
        "                  file_name=\"streamlit_variables.pdf\",\n",
        "                  mime=\"application/pdf\"\n",
        "              )  \n",
        "      st.success('Patient disease is {}'.format(result))\n",
        "      print(result)\n",
        "    \n",
        "\n",
        "\n",
        "# Disease Classification page\n",
        "if (selected == 'Disease Classification'):\n",
        "    st.title('Disease Classification using Deep Learning')\n",
        "    Patient_id =st.text_input(\"Enter your Patient Id\")\n",
        "    gender = st.selectbox(\"Enter your gender\",[\"Male\", \"Female\"])\n",
        "    #col1, col2, col3 = st.columns(3)\n",
        "    age = st.text_input(\"Enter your age\")\n",
        "    #stroke = col1.selectbox(\"Have you ever experienced a seizure?\",[\"Yes\",\"No\"])\n",
        "    disease_sample = st.file_uploader(\"Upload the eeg (EDF)\")\n",
        "    if disease_sample is not None :\n",
        "      with open(os.path.join(\"/content/disease_sample\",\"patient.edf\"),\"wb\") as f: \n",
        "            f.write(disease_sample.getbuffer())       \n",
        "    \n",
        "    def prediction(dataframe):   \n",
        "        loaded_model = load_model('/content/disease_types.h5')\n",
        "        prediction3 = loaded_model.predict(dataframe)\n",
        "        predict_classes = np.argmax(prediction3,axis=1)\n",
        "        prediction4 = statistics.mode(predict_classes)\n",
        "        print(prediction4)\n",
        "        if (prediction4 == 0):\n",
        "            pred = 'Epileptic Patient'\n",
        "        elif (prediction4 == 1):\n",
        "            pred = 'Normal Patient'\n",
        "        elif (prediction4 == 2):\n",
        "            pred = 'intracerebral hemorrhage Patient'\n",
        "        elif (prediction4 == 3):\n",
        "            pred = 'Alzheimeric Patient'\n",
        "        elif (prediction4 == 4):\n",
        "            pred = 'insomniac Patient'\n",
        "        return pred\n",
        "\n",
        "    if st.button(\"Predict\"):\n",
        "      st.write(\"Processing Please Wait\") \n",
        "      s1= mne.io.read_raw_edf('/content/disease_sample/patient.edf',include=['C3','C4','Cz','F3','F4','F7','F8','Fz','Fp1','Fp2','A1','A2','O1',\n",
        "      'O2','P3','P4','T5','T6','Pz','T3','T4','ECG-LA','ECG-RA'])\n",
        "      s2 = mne.export.export_raw('/content/patient1.edf',s1,overwrite=True)\n",
        "      helper= EDFHelper(\"/content/patient1.edf\",lowpass=70, highpass=1, normalize=True, ICA=True,windowSize = 20)\n",
        "      wrapper=eeglib.wrapper.Wrapper(helper)\n",
        "      wrapper.addFeature.PFD()\n",
        "      wrapper.addFeature.DTW((0,1))\n",
        "      wrapper.addFeature.hjorthActivity()\n",
        "      data=wrapper.getAllFeatures()\n",
        "      data.to_csv('patient.csv')\n",
        "      dataframe1 = pd.read_csv('patient.csv')\n",
        "      dataframe1 = dataframe1.drop(dataframe1.columns[0],axis=1)\n",
        "      dataframe1.columns = ['PFD0', 'PFD1', 'PFD2', 'PFD3', 'PFD4', 'PFD5', 'PFD6', 'PFD7', 'PFD8','PFD9', 'PFD10', 'PFD11', 'PFD12', 'PFD13', 'PFD14', 'PFD15', 'PFD16',\n",
        "                           'PFD17', 'PFD18', 'PFD19', 'PFD20', 'PFD21', 'PFD22', 'DTW','hjorthActivity0', 'hjorthActivity1', 'hjorthActivity2',\n",
        "                           'hjorthActivity3', 'hjorthActivity4', 'hjorthActivity5','hjorthActivity6', 'hjorthActivity7', 'hjorthActivity8',\n",
        "                           'hjorthActivity9', 'hjorthActivity10', 'hjorthActivity11','hjorthActivity12', 'hjorthActivity13', 'hjorthActivity14',\n",
        "                            'hjorthActivity15', 'hjorthActivity16', 'hjorthActivity17','hjorthActivity18', 'hjorthActivity19', 'hjorthActivity20',\n",
        "                              'hjorthActivity21', 'hjorthActivity22']\n",
        "      dataframe1.to_csv(\"patient_app.csv\")\n",
        "      dataframe3 = pd.read_csv('patient_app.csv')\n",
        "      dataframe2 = dataframe3.drop(dataframe3.columns[0],axis=1)\n",
        "      scaler.fit(dataframe2)\n",
        "      dataframe2 = scaler.transform(dataframe2)\n",
        "      dataframe = np.array(dataframe2).reshape((dataframe2.shape[0],1,47))\n",
        "      result = prediction(dataframe) \n",
        "      variables = {\"Name\": Patient_id, \"Age\": age, \"Gender\":gender, \"Result\":result} \n",
        "      pdf_file_path = generate_pdf_report(variables)\n",
        "      with st.expander(\"Download PDF Report\"):\n",
        "              st.write(\"Click the button below to download the PDF report\")\n",
        "              st.download_button(\n",
        "                  label=\"Download Report\",\n",
        "                  data=open(pdf_file_path, \"rb\").read(),\n",
        "                  file_name=\"streamlit_variables.pdf\",\n",
        "                  mime=\"application/pdf\"\n",
        "              )  \n",
        "      st.success('Patient disease is {}'.format(result))\n",
        "      print(result)\n",
        "\n",
        "\n",
        "# Early Detection Page\n",
        "if (selected == 'Early Detection'):\n",
        "    \n",
        "    # page title\n",
        "    st.title('Early Detection of Epilepsy using EEG')\n",
        "    \n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    \n",
        "    with col1:\n",
        "        age = st.text_input('Age')\n",
        "        \n",
        "    with col2:\n",
        "        sex = st.text_input('Sex')\n",
        "        \n",
        "    with col3:\n",
        "        cp = st.text_input('Chest Pain types')\n",
        "        \n",
        "    with col1:\n",
        "        trestbps = st.text_input('Resting Blood Pressure')\n",
        "        \n",
        "    with col2:\n",
        "        chol = st.text_input('Serum Cholestoral in mg/dl')\n",
        "        \n",
        "    with col3:\n",
        "        fbs = st.text_input('Fasting Blood Sugar > 120 mg/dl')\n",
        "        \n",
        "    with col1:\n",
        "        restecg = st.text_input('Resting Electrocardiographic results')\n",
        "        \n",
        "    with col2:\n",
        "        thalach = st.text_input('Maximum Heart Rate achieved')\n",
        "        \n",
        "    with col3:\n",
        "        exang = st.text_input('Exercise Induced Angina')\n",
        "        \n",
        "    with col1:\n",
        "        oldpeak = st.text_input('ST depression induced by exercise')\n",
        "        \n",
        "    with col2:\n",
        "        slope = st.text_input('Slope of the peak exercise ST segment')\n",
        "        \n",
        "    with col3:\n",
        "        ca = st.text_input('Major vessels colored by flourosopy')\n",
        "        \n",
        "    with col1:\n",
        "        thal = st.text_input('thal: 0 = normal; 1 = fixed defect; 2 = reversable defect')\n",
        "        \n",
        "    # code for Prediction\n",
        "    early_detection = ''\n",
        "    \n",
        "    # creating a button for Prediction    \n",
        "    if st.button(\"Epilepsy Classification\"):\n",
        "        epilepsy_prediction = prediction_model.predict([[PFD0,PFD1,PFD2,PFD3,PFD4,PFD5,PFD6,PFD7,PFD8,PFD9,PFD10,PFD11,PFD12,PFD13,PFD14,PFD15,PFD16,PFD17,PFD18,PFD19,PFD20,PFD21,PFD22,DTW,hjorthActivity0,hjorthActivity1,hjorthActivity2,hjorthActivity3,hjorthActivity4,hjorthActivity5,hjorthActivity6,hjorthActivity7,hjorthActivity8,hjorthActivity9,hjorthActivity10,hjorthActivity11,hjorthActivity12,hjorthActivity13,hjorthActivity14,hjorthActivity15,hjorthActivity16,hjorthActivity17,hjorthActivity18,hjorthActivity19,hjorthActivity20,hjorthActivity21,hjorthActivity22]])                          \n",
        "        \n",
        "\n",
        "        if (epilepsy_prediction[0] == 0):\n",
        "          early_detection = \"Preictal\"\n",
        "        elif (epilepsy_prediction[0] == 1):\n",
        "          early_detection = \"Interictal\"\n",
        "        elif (epilepsy_prediction[0] == 2):\n",
        "          early_detection = \"Ictal\"\n",
        "        else:\n",
        "          early_detection = \"No stage Found\"\n",
        "        \n",
        "    st.success(early_detection)        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8totSsw_vYxj"
      },
      "outputs": [],
      "source": [
        "! pip install streamlit -q\n",
        "! pip install streamlit_option_menu\n",
        "! pip install --upgrade streamlit\n",
        "! pip install pyngrok\n",
        "!pip install jedi\n",
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqDEnIydUHR8"
      },
      "outputs": [],
      "source": [
        "# ngrok.kill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BYJRwjxylDb",
        "outputId": "44317f2a-094d-46f0-8649-de2f9f234c91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-05-22T08:45:04+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"https://6d86-34-141-252-92.ngrok.io\" -> \"http://localhost:8501\">"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        " \n",
        "public_url = ngrok.connect('8501')\n",
        "public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5IWaS-1yiGP"
      },
      "outputs": [],
      "source": [
        "!streamlit run app1.py &>/dev/null&"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJVusoQGebAs",
        "outputId": "44561a21-e85d-442f-ed1e-76deb7642eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35.231.209.108\n"
          ]
        }
      ],
      "source": [
        "!curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Zhmtb_Yby1F",
        "outputId": "4566f526-a2fe-45f6-a171-49d5960dc9de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[##................] \\ fetchMetadata: sill resolveWithNewModule color-convert@2\u001b[0m\u001b[K\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.231.209.108:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.11s\n",
            "your url is: https://spicy-rabbits-glow.loca.lt\n",
            "2023-05-22 21:09:04.551281: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Extracting EDF parameters from /content/disease_sample/patient.edf...\n",
            "EDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "/content/app1.py:140: RuntimeWarning: Omitted 27 annotation(s) that were outside data range.\n",
            "  s1= mne.io.read_raw_edf('/content/disease_sample/patient.edf',include=['C3','C4','Cz','F3','F4','F7','F8','Fz','Fp1','Fp2','A1','A2','O1','O2',\n",
            "Reading 0 ... 413183  =      0.000 ...  1613.996 secs...\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_fastica.py:542: FutureWarning: Starting in v1.3, whiten='unit-variance' will be used by default.\n",
            "  warnings.warn(\n",
            "646/646 [==============================] - 2s 2ms/step\n",
            "2\n",
            "Delta Generalised Epilepsy\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app1.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}